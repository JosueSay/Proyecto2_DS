model_name: deberta-v3-large
pretrained_name: microsoft/deberta-v3-large

# --- Tokenización ---
max_len_prompt: 128
max_len_resp: 352

# --- Entrenamiento ---
compile: false
epochs: 6
batch_size: 16
grad_accum: 2
lr: 2e-5
weight_decay: 0.01
warmup_ratio: 0.05
scheduler: cosine           # o 'linear'
clip_norm: 1.0
seed: 42
amp: bf16                   # 'bf16' | 'fp16' | 'false'
num_workers: 0
dropout: 0.2
grad_checkpointing: true

# --- Early Stopping ---
early_stopping:
  metric: macro_f1
  mode: max
  patience: 2

# --- Datos ---
data:
  train_csv: data/clean/train_strat.csv
  valid_csv: data/clean/valid_strat.csv
  test_csv: data/clean/test_clean.csv
  use_label: true
  use_clean_cols: true
  shuffle: true
  val_batch_size: 32
  pin_memory: true

# --- Pérdida ---
loss:
  type: cross_entropy
  label_smoothing: 0.05

# --- Logging/Salidas ---
logging:
  reports_dir: reports/deberta
  runs_dir: runs/deberta_pairranker
  step_csv: train_history.csv
  epoch_csv: epoch_metrics.csv
  alerts_csv: alerts.csv
  confusion_csv: val_confusion.csv
  class_report_csv: val_class_report.csv
  preds_sample_csv: val_preds_sample.csv
  step_interval: 20

# --- Monitoreo/Checkpoints ---
monitor:
  detect_collapse: true
  save_best_by: macro_f1
  save_last: true
  verbose: true

# --- Entorno / Tokenizador ---
env:
  tokenizers_parallelism: true
  cuda_launch_blocking: 0
  pytorch_cuda_alloc_conf: max_split_size_mb:128
  hf_home: .hf_cache
  use_slow_tokenizer: false
