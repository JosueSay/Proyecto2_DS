# modelo base
model:
  name: roberta-base
  pretrained_name: roberta-base
  dropout: 0.15
  grad_checkpointing: true
  compile: false

# longitudes de entrada
lengths:
  max_len_prompt: 96
  max_len_resp: 320

# entrenamiento
train:
  epochs: 6
  batch_size: 8
  grad_accum: 2
  lr: 2e-5
  weight_decay: 0.01
  warmup_ratio: 0.06
  scheduler: cosine
  clip_norm: 2.5
  seed: 42
  amp: "false"
  num_workers: 8

# datos
data:
  train_csv: data/clean/train_strat.csv
  valid_csv: data/clean/valid_strat.csv
  use_label: true
  use_clean_cols: true
  shuffle: true
  val_batch_size: 32
  pin_memory: true
  prompt_col: prompt_clean
  respA_col: response_a_clean
  respB_col: response_b_clean

# dataloader
dataloader:
  prefetch_factor_train: 6
  prefetch_factor_val: 6
  persistent_workers: true

# pérdida y regularización
loss:
  type: "bradley_terry"
  label_smoothing: 0.10
  class_weights: [1.0, 1.0, 1.9]

# evaluación e inferencia
eval:
  bt_temp: 0.80
  tie_tau: 0.60
  tie_alpha: 0.80

# logging y salidas
logging:
  reports_dir: reports/roberta
  runs_dir: results/roberta
  step_csv: steps.csv
  epoch_csv: epochs.csv
  alerts_csv: alerts.csv
  confusion_csv: confusion.csv
  class_report_csv: class_report.csv
  preds_sample_csv: preds.csv
  pred_distributions_csv: pred_distributions.csv
  val_pred_tpl: val_predictions_epoch_{:02}.csv
  token_budget_tpl: token_budget_epoch_{:02}.csv
  run_config_used: run_config_used.yaml
  step_interval: 200

# monitoreo/checkpoints
monitor:
  detect_collapse: true
  save_best_by: macro_f1
  save_last: true
  verbose: true

# early stopping
early_stopping:
  metric: macro_f1
  mode: max
  patience: 3

# entorno
env:
  tokenizers_parallelism: true
  cuda_launch_blocking: 0
  pytorch_cuda_alloc_conf: "max_split_size_mb:128"
  hf_home: ".hf_cache"
  use_slow_tokenizer: false
